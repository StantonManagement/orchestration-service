# Story 3.2: Metrics & Monitoring

## Status
Approved

## Story
**As a** system administrator,
**I want** to monitor system performance and throughput metrics,
**so that** I can ensure the service is meeting performance requirements and identify issues proactively.

## Acceptance Criteria
1. GET `/orchestrate/metrics` endpoint implementation
2. Metrics collection for SMS volume, response times, approval rates, and escalations
3. Structured logging with correlation IDs for request tracing
4. Performance monitoring for processing times and throughput
5. Dashboard-ready metrics format for monitoring systems

## Tasks / Subtasks
- [ ] Task 1: Create metrics collection service (AC: 2, 4)
  - [ ] Create `app/services/metrics_service.py` with MetricsService class
  - [ ] Implement SMS volume tracking by hour/day/week
  - [ ] Add response time collection and aggregation
  - [ ] Create approval rate calculation metrics
  - [ ] Implement escalation tracking and categorization
  - [ ] Add payment plan detection metrics
  - [ ] Create throughput and performance monitoring
- [ ] Task 2: Enhance structured logging with correlation IDs (AC: 3)
  - [ ] Review and enhance existing `app/core/logging.py` implementation
  - [ ] Add correlation ID generation and propagation middleware
  - [ ] Implement request tracing across all service calls
  - [ ] Add structured logging for all key business events
  - [ ] Create log aggregation and filtering capabilities
  - [ ] Add performance logging with timing measurements
  - [ ] Implement log sampling for high-volume scenarios
- [ ] Task 3: Create metrics API endpoint (AC: 1, 5)
  - [ ] Create `app/api/metrics.py` with metrics router
  - [ ] Implement GET /orchestrate/metrics endpoint
  - [ ] Create `app/schemas/metrics.py` with MetricsResponse schemas
  - [ ] Add time range filtering for metrics queries
  - [ ] Implement dashboard-ready JSON format
  - [ ] Add metrics caching for performance optimization
  - [ ] Include system health indicators in metrics response
- [ ] Task 4: Implement performance monitoring integration (AC: 4)
  - [ ] Add performance timing to all critical service operations
  - [ ] Implement database query performance monitoring
  - [ ] Create external service call timing metrics
  - [ ] Add workflow step performance tracking
  - [ ] Implement throughput calculations and baselines
  - [ ] Create performance alerting thresholds
  - [ ] Add memory and CPU utilization monitoring
- [ ] Task 5: Integrate metrics collection into existing services (AC: 2)
  - [ ] Update orchestration service to log SMS processing metrics
  - [ ] Integrate metrics into AI service for response time tracking
  - [ ] Add approval workflow metrics to approval service
  - [ ] Update escalation service with escalation metrics
  - [ ] Integrate payment plan service with detection metrics
  - [ ] Add workflow service performance monitoring
  - [ ] Update all external service clients with performance metrics
- [ ] Task 6: Create monitoring dashboard integration (AC: 5)
  - [ ] Design Prometheus-compatible metrics format
  - [ ] Create Grafana dashboard template configurations
  - [ ] Add real-time metrics streaming capabilities
  - [ ] Implement metrics aggregation and rollup functions
  - [ ] Create alert rule definitions for critical metrics
  - [ ] Add historical data retention and archiving
  - [ ] Implement metrics export capabilities
- [ ] Task 7: Create comprehensive tests (Test Strategy compliance)
  - [ ] Create `tests/test_services/test_metrics_service.py` for metrics logic
  - [ ] Create `tests/test_api/test_metrics.py` for API endpoint tests
  - [ ] Create `tests/test_core/test_enhanced_logging.py` for logging tests
  - [ ] Test metrics collection accuracy with various scenarios
  - [ ] Test correlation ID propagation across service boundaries
  - [ ] Test performance monitoring under different load conditions
  - [ ] Test dashboard format compatibility with monitoring systems

## Dev Notes

### Previous Story Insights
From Story 3.1 (Error Handling & Circuit Breakers):
- Circuit breaker implementation provides failure metrics and service health data
- Enhanced exception hierarchy supports error rate monitoring
- Structured logging foundation is established for correlation ID tracking
- Service health check endpoints provide availability metrics

From Story 1.5 (Workflow Status Tracking):
- Comprehensive workflow step tracking provides detailed performance data
- Audit trail logging creates timing and status change metrics
- Database schema includes timing fields for performance analysis
- Correlation ID propagation is implemented across workflow operations

From Stories 2.1 & 2.2 (Payment Plan Detection & Escalation Logic):
- Payment plan validation provides business rule metrics
- Escalation trigger detection creates escalation categorization data
- Timeout monitoring provides response time metrics
- Integration patterns established for metrics collection points

### Existing Infrastructure Assessment
**Current State of Monitoring Infrastructure**:
- `app/core/logging.py`: Basic structured logging exists (needs enhancement for correlation IDs)
- `app/utils/timeout_monitor.py`: Timeout monitoring exists (can be leveraged for performance metrics)
- Circuit breaker implementation: Provides failure metrics and service health data
- Workflow step tracking: Creates detailed timing and status data

**Enhancement Opportunities**:
- Logging needs correlation ID propagation middleware
- Metrics service needs comprehensive business metrics collection
- Performance monitoring needs integration points across all services
- Dashboard integration needs Prometheus-compatible format

### API Specifications
**GET /orchestrate/metrics Endpoint** (from `docs/architecture/rest-api-spec.md`):
- **Purpose**: Get system metrics
- **Response**: 200 OK with comprehensive system metrics
- **Response Format**:
  ```json
  {
    "last_hour": {
      "sms_received": integer,
      "ai_responses": integer,
      "auto_approval_rate": number,
      "avg_response_time_ms": integer
    },
    "today": {
      "total_messages": integer,
      "escalations": integer,
      "payment_plans": integer
    }
  }
  ```

### Metrics Collection Requirements
**Business Metrics to Track**:
- **SMS Volume**: Messages received, processed, failed
- **Response Times**: AI generation, external service calls, total processing
- **Approval Rates**: Auto-approval, manual approval, escalation rates
- **Escalations**: By type, severity, resolution time
- **Payment Plans**: Detection rate, validation results, auto-approval
- **Workflow Performance**: Step completion times, failure rates, recovery times

**Performance Metrics to Track**:
- **Processing Times**: Database queries, external service calls, AI generation
- **Throughput**: Messages per minute, concurrent processing capacity
- **Resource Utilization**: Memory usage, CPU usage, connection pooling
- **Error Rates**: By error type, service, time period
- **Circuit Breaker Status**: Service availability, failure counts, state transitions

### Structured Logging Requirements
**From `docs/architecture/error-handling-strategy.md`**:
- **Library**: structlog 23+ (already implemented)
- **Format**: JSON with correlation ID, timestamp, level, message, context
- **Required Context**:
  - Correlation ID: UUID per request for tracing
  - Service Context: service name, version, environment
  - User Context: tenant_id, conversation_id, workflow_id

**Enhanced Logging Requirements**:
- **Request Tracing**: Correlation ID propagation across all service boundaries
- **Performance Logging**: Timing measurements for all critical operations
- **Business Event Logging**: Structured logging for key business events
- **Error Logging**: Detailed error context with stack traces and user impact

### File Locations
Based on `docs/architecture/source-tree.md` and existing structure:
- **Metrics Service**: `app/services/metrics_service.py` (new)
- **Metrics API**: `app/api/metrics.py` (new)
- **Metrics Schemas**: `app/schemas/metrics.py` (new)
- **Enhanced Logging**: `app/core/logging.py` (enhance existing)
- **Service Integration**: Update existing services with metrics collection
- **Tests**:
  - `tests/test_services/test_metrics_service.py`
  - `tests/test_api/test_metrics.py`
  - `tests/test_core/test_enhanced_logging.py`

### Technology Stack
**From `docs/architecture/tech-stack.md`**:
- **Monitoring**: Prometheus + structlog 1.0+ / 23+ for metrics/logging
- **Metrics Collection**: Custom service with time-series data aggregation
- **Performance Monitoring**: Built-in timing and throughput calculations
- **Structured Logging**: structlog with correlation ID propagation
- **Dashboard Integration**: Prometheus-compatible format for Grafana

### Performance Monitoring Strategy
**Key Performance Indicators (KPIs)**:
- **Response Times**: P50, P90, P99 percentiles for all operations
- **Throughput**: Messages processed per minute/hour/day
- **Error Rates**: By error category and service
- **Availability**: Service uptime and circuit breaker status
- **Resource Utilization**: Memory, CPU, database connections

**Monitoring Integration Points**:
- **Database Operations**: Query execution times and connection pooling
- **External Service Calls**: HTTP request times and failure rates
- **AI Processing**: Response generation times and API call metrics
- **Workflow Steps**: Individual step timing and completion rates
- **Business Logic**: Payment plan validation, approval processing, escalation handling

### Dashboard Integration Requirements
**Prometheus Compatibility**:
- Metrics in Prometheus exposition format
- Gauge and counter metric types as appropriate
- Histogram support for response time distributions
- Labels for service, operation, status dimensions

**Grafana Dashboard Templates**:
- Real-time system overview with key metrics
- Detailed service performance breakdowns
- Historical trend analysis and comparison
- Alert threshold visualization and configuration
- Business metrics dashboards for operational teams

### Technical Constraints
**From `docs/architecture/coding-standards.md`**:
- Use structured logging with correlation IDs for all tracing
- Always validate external inputs using Pydantic models
- Database operations through service layer, never direct from API routes
- Follow existing error handling patterns with proper status codes
- Use async/await patterns for all external service calls
- Implement proper timeout handling for external service calls
- Add performance timing to all critical operations without blocking

### Configuration Requirements
**Metrics Configuration**:
- Retention periods for different metric types
- Aggregation intervals (1min, 5min, 1hour, 1day)
- Sampling rates for high-volume metrics
- Alert thresholds for critical metrics

**Logging Configuration**:
- Log levels for different environments
- Correlation ID generation strategy
- Performance sampling rates
- Log retention and archival policies

### Integration with Existing Services
**Service-Specific Metrics**:
- **Orchestration Service**: SMS processing volume and timing
- **AI Service**: Response generation times and confidence scores
- **Approval Service**: Approval rates and processing times
- **Escalation Service**: Escalation volumes and resolution times
- **Payment Plan Service**: Detection rates and validation results
- **External Service Clients**: Call times, failure rates, circuit breaker status

### Monitoring and Observability
**Real-time Monitoring**:
- Service health and availability metrics
- Response time and throughput monitoring
- Error rate tracking and alerting
- Resource utilization monitoring

**Historical Analysis**:
- Trend analysis for performance metrics
- Seasonal pattern detection in business metrics
- Capacity planning based on growth trends
- Performance optimization opportunities

### Testing Strategy
**From `docs/architecture/test-strategy-and-standards.md`**:
- **Framework**: pytest 7.4+ with pytest-asyncio
- **Coverage**: 80% line coverage for metrics collection paths
- **Mocking**: pytest-mock for time-series data and external dependencies
- **Test Types**: Unit tests for metrics logic, Integration tests for API endpoints

**Specific Testing Requirements**:
- Test metrics collection accuracy with various scenarios
- Test correlation ID propagation across service boundaries
- Test performance monitoring under different load conditions
- Test dashboard format compatibility with monitoring systems
- Test metrics aggregation and rollup functions
- Test alert threshold configuration and triggering

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-03 | 1.0 | Initial story creation | Scrum Master |

## Dev Agent Record

### Agent Model Used
<!-- To be populated by Dev Agent -->

### Debug Log References
<!-- To be populated by Dev Agent -->

### Completion Notes List
<!-- To be populated by Dev Agent -->

### File List
<!-- To be populated by Dev Agent -->

## QA Results
<!-- To be populated by QA Agent -->